{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForTokenClassification, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook, will introduce how to do NER with BERT, including:\n",
    "\n",
    "* Load and preprocess data\n",
    "* Parser data\n",
    "* Make training data\n",
    "* Train model\n",
    "* Evaluate result\n",
    "* Inference\n",
    "\n",
    "Tips:\n",
    "\n",
    "* Update to transformer==2.5.1\n",
    "* When come across OOV,you will find that BERT word piece tokenize method can help a lot\n",
    "* Case model will be litter better than uncase model for English\n",
    "\n",
    "Also this notebook come with a post NER with BERT in Action (https://medium.com/@yingbiao/ner-with-bert-in-action-936ff275bc73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fillna method can make same sentence with same sentence name\n",
    "df_data = pd.read_csv(\"D:/DataAnalysis/Python/BERT/ner_dataset.csv/ner_dataset.csv\",sep=\",\",encoding=\"latin1\").fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>conflict</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>joined</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>protesters</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>who</td>\n",
       "      <td>WP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>carried</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>banners</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>such</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>slogans</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1   Sentence: 1             of   IN      O\n",
       "2   Sentence: 1  demonstrators  NNS      O\n",
       "3   Sentence: 1           have  VBP      O\n",
       "4   Sentence: 1        marched  VBN      O\n",
       "5   Sentence: 1        through   IN      O\n",
       "6   Sentence: 1         London  NNP  B-geo\n",
       "7   Sentence: 1             to   TO      O\n",
       "8   Sentence: 1        protest   VB      O\n",
       "9   Sentence: 1            the   DT      O\n",
       "10  Sentence: 1            war   NN      O\n",
       "11  Sentence: 1             in   IN      O\n",
       "12  Sentence: 1           Iraq  NNP  B-geo\n",
       "13  Sentence: 1            and   CC      O\n",
       "14  Sentence: 1         demand   VB      O\n",
       "15  Sentence: 1            the   DT      O\n",
       "16  Sentence: 1     withdrawal   NN      O\n",
       "17  Sentence: 1             of   IN      O\n",
       "18  Sentence: 1        British   JJ  B-gpe\n",
       "19  Sentence: 1         troops  NNS      O\n",
       "20  Sentence: 1           from   IN      O\n",
       "21  Sentence: 1           that   DT      O\n",
       "22  Sentence: 1        country   NN      O\n",
       "23  Sentence: 1              .    .      O\n",
       "24  Sentence: 2       Families  NNS      O\n",
       "25  Sentence: 2             of   IN      O\n",
       "26  Sentence: 2       soldiers  NNS      O\n",
       "27  Sentence: 2         killed  VBN      O\n",
       "28  Sentence: 2             in   IN      O\n",
       "29  Sentence: 2            the   DT      O\n",
       "30  Sentence: 2       conflict   NN      O\n",
       "31  Sentence: 2         joined  VBD      O\n",
       "32  Sentence: 2            the   DT      O\n",
       "33  Sentence: 2     protesters  NNS      O\n",
       "34  Sentence: 2            who   WP      O\n",
       "35  Sentence: 2        carried  VBD      O\n",
       "36  Sentence: 2        banners  NNS      O\n",
       "37  Sentence: 2           with   IN      O\n",
       "38  Sentence: 2           such   JJ      O\n",
       "39  Sentence: 2        slogans  NNS      O"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(n=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence #', 'Word', 'POS', 'Tag'], dtype='object')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look POS cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.POS.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look TAG cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\n",
       "       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\n",
       "       'I-eve', 'I-nat'], dtype=object)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.Tag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47959, 35178, 42, 17)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse summary of data\n",
    "df_data['Sentence #'].nunique(), df_data.Word.nunique(), df_data.POS.nunique(), df_data.Tag.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse the Tag distribution\n",
    "df_data.Tag.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain tag\n",
    "As show above, there are two parts for the tag name: \"position\"-\"meaning\"\n",
    "\n",
    "* B: begin, word at the first position\n",
    "* I: middle, word not at the first position,especially for phase\n",
    "* time: time, meaning time\n",
    "* per: person, meaning people name\n",
    "* geo: geography, meaning location name\n",
    "* O: mean other, set as a default tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser data\n",
    "Parser data into document structure\n",
    "\n",
    "Since we will treat the NER process as a multi-class classification, we need to make the training data into “token-label” form, we need to parser the sentence from data set. So that we can get lists of token and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets get lists of token and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full document data structure\n",
    "getter = SentenceGetter(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'London',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'and',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'withdrawal',\n",
       " 'of',\n",
       " 'British',\n",
       " 'troops',\n",
       " 'from',\n",
       " 'that',\n",
       " 'country',\n",
       " '.']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sentence data\n",
    "sentences = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Iranian',\n",
       " 'officials',\n",
       " 'say',\n",
       " 'they',\n",
       " 'expect',\n",
       " 'to',\n",
       " 'get',\n",
       " 'access',\n",
       " 'to',\n",
       " 'sealed',\n",
       " 'sensitive',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'plant',\n",
       " 'Wednesday',\n",
       " ',',\n",
       " 'after',\n",
       " 'an',\n",
       " 'IAEA',\n",
       " 'surveillance',\n",
       " 'system',\n",
       " 'begins',\n",
       " 'functioning',\n",
       " '.']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pos data\n",
    "#poses = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "#print(poses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Get tag labels data\n",
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make TAG name into index for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = list(set(df_data[\"Tag\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add X  label for word piece support\n",
    "# Add [CLS] and [SEP] as BERT need\n",
    "tags_vals.append('X')\n",
    "tags_vals.append('[CLS]')\n",
    "tags_vals.append('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = set(tags_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'X',\n",
       " '[CLS]',\n",
       " '[SEP]'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a dict for mapping id to tag name\n",
    "#tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "# Recommend to set it by manual define, good for reusing\n",
    "tag2idx={'B-art': 14,\n",
    " 'B-eve': 16,\n",
    " 'B-geo': 0,\n",
    " 'B-gpe': 13,\n",
    " 'B-nat': 12,\n",
    " 'B-org': 10,\n",
    " 'B-per': 4,\n",
    " 'B-tim': 2,\n",
    " 'I-art': 5,\n",
    " 'I-eve': 7,\n",
    " 'I-geo': 15,\n",
    " 'I-gpe': 8,\n",
    " 'I-nat': 11,\n",
    " 'I-org': 3,\n",
    " 'I-per': 6,\n",
    " 'I-tim': 1,\n",
    " 'X':17,\n",
    " 'O': 9,\n",
    " '[CLS]':18,\n",
    " '[SEP]':19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art': 14,\n",
       " 'B-eve': 16,\n",
       " 'B-geo': 0,\n",
       " 'B-gpe': 13,\n",
       " 'B-nat': 12,\n",
       " 'B-org': 10,\n",
       " 'B-per': 4,\n",
       " 'B-tim': 2,\n",
       " 'I-art': 5,\n",
       " 'I-eve': 7,\n",
       " 'I-geo': 15,\n",
       " 'I-gpe': 8,\n",
       " 'I-nat': 11,\n",
       " 'I-org': 3,\n",
       " 'I-per': 6,\n",
       " 'I-tim': 1,\n",
       " 'X': 17,\n",
       " 'O': 9,\n",
       " '[CLS]': 18,\n",
       " '[SEP]': 19}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping index to name\n",
    "tag2name={tag2idx[key] : key for key in tag2idx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14: 'B-art',\n",
       " 16: 'B-eve',\n",
       " 0: 'B-geo',\n",
       " 13: 'B-gpe',\n",
       " 12: 'B-nat',\n",
       " 10: 'B-org',\n",
       " 4: 'B-per',\n",
       " 2: 'B-tim',\n",
       " 5: 'I-art',\n",
       " 7: 'I-eve',\n",
       " 15: 'I-geo',\n",
       " 8: 'I-gpe',\n",
       " 11: 'I-nat',\n",
       " 3: 'I-org',\n",
       " 6: 'I-per',\n",
       " 1: 'I-tim',\n",
       " 17: 'X',\n",
       " 9: 'O',\n",
       " 18: '[CLS]',\n",
       " 19: '[SEP]'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make training data\n",
    "Make raw data into trainable data for BERT, including:\n",
    "\n",
    "1) Set gpu environment\n",
    "\n",
    "2) Load tokenizer and tokenize\n",
    "\n",
    "3) Set 3 embedding, token embedding, mask word embedding, segmentation embedding\n",
    "\n",
    "4) Split data set into train and validate, then send them to dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data\n",
    "\n",
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure:\n",
    "\n",
    "we get a tokenizer that corresponds to the model architecture we want to use, we download the vocabulary used when pretraining this specific checkpoint. That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Set up gpu environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "n_cpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Load tokenizer\n",
    "You can download the tokenizer file into local folder first :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual define vocabulary address, if you download the tokenzier file in local\n",
    "# vocab.txt, download from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\n",
    "vocabulary = \"D:/DataAnalysis/Python/BERT/bert-base-cased-vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Len of the sentence must be not bigger than the training model\n",
    "# See model's 'max_position_embeddings' = 512\n",
    "max_len  = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer, with manual file address or pretrained address\n",
    "tokenizer=BertTokenizer(vocab_file=vocabulary,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer text\n",
    "\n",
    "1) In hunggieface for bert, when come across OOV, will word piece the word\n",
    "\n",
    "Note: Note that transformers are often pretrained with subword tokenizers, meaning that even if your inputs have been split into words already, each of those words could be split again by the tokenizer. Let's look at an example of that as shown in below fig.\n",
    "\n",
    "Note: This means that we need to do some processing on our labels as the input ids returned by the tokenizer are longer than the lists of labels our dataset contain, first because some special tokens might be added (we can a [CLS] and a [SEP] above) and then because of those possible splits of words in multiple tokens:\n",
    "\n",
    "2) We need to adjust the labels base on the tokenize result, “##abc” need to set label \"X\"\n",
    "\n",
    "3) Need to set \"[CLS]\" at front and \"[SEP]\" at the end, as what the paper do, BERT indexer should add [CLS] and [SEP] tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 257,
     "metadata": {
      "image/png": {
       "height": 1000,
       "width": 1000
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = 'D:/DataAnalysis/Python/BERT/Capture1.PNG', width = 1000, height = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0,len:28\n",
      "texts:[CLS] Thousands of demons ##tra ##tors have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country . [SEP]\n",
      "No.0,len:28\n",
      "lables:[CLS] O O O X X O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O [SEP]\n",
      "No.1,len:29\n",
      "texts:[CLS] Iranian officials say they expect to get access to sealed sensitive parts of the plant Wednesday , after an I ##A ##EA surveillance system begins functioning . [SEP]\n",
      "No.1,len:29\n",
      "lables:[CLS] B-gpe O O O O O O O O O O O O O O B-tim O O O B-org X X O O O O O [SEP]\n",
      "No.2,len:44\n",
      "texts:[CLS] He ##lic ##op ##ter guns ##hips Saturday pounded militant hide ##outs in the Or ##ak ##zai tribal region , where many Taliban militants are believed to have fled to avoid an earlier military offensive in nearby South W ##azi ##rist ##an . [SEP]\n",
      "No.2,len:44\n",
      "lables:[CLS] O X X X O X B-tim O O O X O O B-geo X X O O O O O B-org O O O O O O O O O O O O O O B-geo I-geo X X X O [SEP]\n",
      "No.3,len:16\n",
      "texts:[CLS] They left after a tense hour - long stand ##off with riot police . [SEP]\n",
      "No.3,len:16\n",
      "lables:[CLS] O O O O O O X X O X O O O O [SEP]\n",
      "No.4,len:47\n",
      "texts:[CLS] U . N . relief coordinator Jan E ##gel ##and said Sunday , U . S . , Indonesian and Australian military helicopters are ferry ##ing out food and supplies to remote areas of western Ace ##h province that ground crews can not reach . [SEP]\n",
      "No.4,len:47\n",
      "lables:[CLS] B-geo X X X O O B-per I-per X X O B-tim O B-geo X X X O B-gpe O B-gpe O O O O X O O O O O O O O O B-geo X O O O O O O O O [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "word_piece_labels = []\n",
    "i_inc = 0\n",
    "for word_list,label in (zip(sentences,labels)):\n",
    "    temp_lable = []\n",
    "    temp_token = []\n",
    "    \n",
    "    # Add [CLS] at the front \n",
    "    temp_lable.append('[CLS]')\n",
    "    temp_token.append('[CLS]')\n",
    "    \n",
    "    for word,lab in zip(word_list,label):\n",
    "        token_list = tokenizer.tokenize(word)\n",
    "        for m,token in enumerate(token_list):\n",
    "            temp_token.append(token)\n",
    "            if m==0:\n",
    "                temp_lable.append(lab)\n",
    "            else:\n",
    "                temp_lable.append('X')  \n",
    "                \n",
    "    # Add [SEP] at the end\n",
    "    temp_lable.append('[SEP]')\n",
    "    temp_token.append('[SEP]')\n",
    "    \n",
    "    tokenized_texts.append(temp_token)\n",
    "    word_piece_labels.append(temp_lable)\n",
    "    \n",
    "    if 5 > i_inc:\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_token)))\n",
    "        print(\"texts:%s\"%(\" \".join(temp_token)))\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_lable)))\n",
    "        print(\"lables:%s\"%(\" \".join(temp_lable)))\n",
    "    i_inc +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set token embedding\n",
    "Pad or trim the text and label to fit the need for max len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101 26159  1104  8568  4487  5067  1138  9639  1194  1498  1106  5641\n",
      "  1103  1594  1107  5008  1105  4555  1103 10602  1104  1418  2830  1121\n",
      "  1115  1583   119   102     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Make text token into id\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18  9  9  9 17 17  9  9  9  0  9  9  9  9  9  0  9  9  9  9  9 13  9  9\n",
      "  9  9  9 19  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n"
     ]
    }
   ],
   "source": [
    "# Make label into id, pad with \"O\" meaning others\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels],\n",
    "                     maxlen=max_len, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set mask word embedding\n",
    "In order to make mask word embedding, we need to use 1 to indicate the real token and 0 to indicate to pad token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# For fine tune of predict, with token mask is 1,pad token is 0\n",
    "attention_masks = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "print(attention_masks[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set segment embedding(Seem like for sequance tagging task, it's not necessary to make this embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since only one sentence, all the segment set to 0\n",
    "segment_ids = [[0] * len(input_id) for input_id in input_ids]\n",
    "segment_ids[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train and validate\n",
    "70% for training, 30% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks,tr_segs, val_segs = train_test_split(input_ids, tags,attention_masks,segment_ids, \n",
    "                                                            random_state=4, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33571, 14388, 33571, 14388)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_inputs),len(val_inputs),len(tr_segs),len(val_segs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set data into tensor\n",
    "\n",
    "Not recommend tensor.to(device) at this process, since it will run out of GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "tr_segs = torch.tensor(tr_segs)\n",
    "val_segs = torch.tensor(val_segs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put data into data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch num\n",
    "batch_num = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only set token embedding, attention embedding, no segment embedding\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Drop last can make batch training better for the last one\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "You can download the model require files into local folder first\n",
    "\n",
    "pytorch_model.bin: pytorch_model.bin\n",
    "\n",
    "config.json: config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this folder, contain model confg(json) and model weight(bin) files\n",
    "# pytorch_model.bin, download from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\n",
    "# config.json, downlaod from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json\n",
    "model_file_address = 'D:/DataAnalysis/Python/BERT/bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:/DataAnalysis/Python/BERT/bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at D:/DataAnalysis/Python/BERT/bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Will load config and weight with from_pretrained()\n",
    "model = BertForTokenClassification.from_pretrained(model_file_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set epoch and grad max num\n",
    "epochs = 5\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cacluate train optimiazaion num\n",
    "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set fine tuning method\n",
    "\n",
    "#### Manual optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True: fine tuning all the layers \n",
    "# False: only fine tuning the classifier layers\n",
    "FULL_FINETUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN loop\n",
    "model.train();\n",
    "\n",
    "b_labels = b_labels.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 33571\n",
      "  Batch size = 32\n",
      "  Num steps = 5250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██████████████▏                                                        | 1/5 [1:52:22<7:29:28, 6742.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1427163745217146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████████████████████████████▍                                          | 2/5 [3:44:35<5:36:58, 6739.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06796318564290542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████████████████████████████████████████▌                            | 3/5 [5:36:59<3:44:41, 6740.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05254879844303041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████████████████████████████████████████████████████▊              | 4/5 [7:23:40<1:50:38, 6638.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0395832733056722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [9:11:51<00:00, 6622.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02997857703515697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        ###############Bug fix code####################\n",
    "        b_input_ids = b_input_ids.type(torch.LongTensor)\n",
    "        b_input_mask = b_input_mask.type(torch.LongTensor)\n",
    "        b_labels = b_labels.type(torch.LongTensor)\n",
    "\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = b_input_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "         ############################################\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "        attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, scores = outputs[:2]\n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_out_address = 'models/bert_out_model/en09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dir if not exits\n",
    "if not os.path.exists(bert_out_address):\n",
    "        os.makedirs(bert_out_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(bert_out_address, \"pytorch_model.bin\")\n",
    "output_config_file = os.path.join(bert_out_address, \"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/bert_out_model/en09\\\\vocab.txt',)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model into file\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(bert_out_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(bert_out_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalue loop\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples =14388\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\Anaconda3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: X seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 socre: 0.833632\n",
      "Accuracy score: 0.971210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\Anaconda3\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Eval results *****\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _     0.0000    0.0000    0.0000         0\n",
      "         art     0.2025    0.1221    0.1524       131\n",
      "         eve     0.3431    0.3846    0.3627        91\n",
      "         geo     0.8594    0.8928    0.8758     11066\n",
      "         gpe     0.9494    0.9480    0.9487      4830\n",
      "         nat     0.6571    0.4694    0.5476        49\n",
      "         org     0.6820    0.7143    0.6978      5954\n",
      "         per     0.7708    0.7882    0.7794      5123\n",
      "         tim     0.8594    0.8775    0.8683      6016\n",
      "\n",
      "   micro avg     0.8226    0.8449    0.8336     33260\n",
      "   macro avg     0.5915    0.5774    0.5814     33260\n",
      "weighted avg     0.8227    0.8449    0.8336     33260\n",
      "\n",
      "f1 socre: 0.833632\n",
      "Accuracy score: 0.971210\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples ={}\".format(len(val_inputs)))\n",
    "print(\"  Batch size = {}\".format(batch_num))\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, label_ids = batch\n",
    "    \n",
    "    #b_input_ids = torch.tensor(b_input_ids).to(torch.int64)\n",
    "    \n",
    "#     if step > 2:\n",
    "#         break\n",
    "\n",
    "    ###############Bug fix code####################\n",
    "    input_ids = input_ids.type(torch.LongTensor)\n",
    "    input_mask = input_mask.type(torch.LongTensor)\n",
    "    label_ids = label_ids.type(torch.LongTensor)\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    labels = label_ids.to(device)\n",
    "    ############################################\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "        attention_mask=input_mask,)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] \n",
    "    \n",
    "    # Get NER predict result\n",
    "    logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    # Get NER true result\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    \n",
    "    \n",
    "    # Only predict the real word, mark=0, will not calculate\n",
    "    input_mask = input_mask.to('cpu').numpy()\n",
    "    \n",
    "    # Compare the valuable predict result\n",
    "    for i,mask in enumerate(input_mask):\n",
    "        # Real one\n",
    "        temp_1 = []\n",
    "        # Predict one\n",
    "        temp_2 = []\n",
    "        \n",
    "        for j, m in enumerate(mask):\n",
    "            # Mark=0, meaning its a pad word, dont compare\n",
    "            if m:\n",
    "                if tag2name[label_ids[i][j]] != \"X\" and tag2name[label_ids[i][j]] != \"[CLS]\" and tag2name[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n",
    "                    temp_1.append(tag2name[label_ids[i][j]])\n",
    "                    temp_2.append(tag2name[logits[i][j]])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "            \n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "# Get acc , recall, F1 result report\n",
    "report = classification_report(y_true, y_pred,digits=4)\n",
    "\n",
    "# Save the report into file\n",
    "output_eval_file = os.path.join(bert_out_address, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    print(\"***** Eval results *****\")\n",
    "    print(\"\\n%s\"%(report))\n",
    "    print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "    print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "    \n",
    "    writer.write(\"f1 socre:\\n\")\n",
    "    writer.write(str(f1_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\nAccuracy score:\\n\")\n",
    "    writer.write(str(accuracy_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\n\")  \n",
    "    writer.write(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "After we trained a model, we can make it into service ---- sending a new sentence to the model,then get the prediction.\n",
    "\n",
    "The process contains:\n",
    "\n",
    "* Load model\n",
    "* Load tokenizer\n",
    "* Set test query (Prediction on new sentences)\n",
    "* Make query into embedding\n",
    "* Predict with model\n",
    "* Parser result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set tag index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag to index, must be the same as we training\n",
    "tag2idx={'B-art': 14,\n",
    " 'B-eve': 16,\n",
    " 'B-geo': 0,\n",
    " 'B-gpe': 13,\n",
    " 'B-nat': 12,\n",
    " 'B-org': 10,\n",
    " 'B-per': 4,\n",
    " 'B-tim': 2,\n",
    " 'I-art': 5,\n",
    " 'I-eve': 7,\n",
    " 'I-geo': 15,\n",
    " 'I-gpe': 8,\n",
    " 'I-nat': 11,\n",
    " 'I-org': 3,\n",
    " 'I-per': 6,\n",
    " 'I-tim': 1,\n",
    " 'X':17,\n",
    " 'O': 9,\n",
    " '[CLS]':18,\n",
    " '[SEP]':19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping index to name\n",
    "tag2name={tag2idx[key] : key for key in tag2idx.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model we trained before, the dir containing pytorch_model.bin and vocab.txt\n",
    "save_model_address = 'models/bert_out_model/en09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = BertForTokenClassification.from_pretrained(save_model_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, our save model address containing pytorch_model.bin and vocab.txt\n",
    "# So, we can load the tokenzier from the same dir as the save model address\n",
    "tokenizer = BertTokenizer.from_pretrained(save_model_address,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max sentence length, must be the same as our training process\n",
    "max_len  = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"My name is Abhijeet, I am from India but currently live in USA, this is my Office laptop.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make query into embeddings\n",
    "\n",
    "* token id embedding, need to tokenize first\n",
    "* mask word embedding\n",
    "* segmentation embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = []\n",
    "temp_token = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add [CLS] at the front \n",
    "temp_token.append('[CLS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = tokenizer.tokenize(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'A',\n",
       " '##b',\n",
       " '##hi',\n",
       " '##jee',\n",
       " '##t',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'from',\n",
       " 'India',\n",
       " 'but',\n",
       " 'currently',\n",
       " 'live',\n",
       " 'in',\n",
       " 'USA',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'my',\n",
       " 'Office',\n",
       " 'laptop',\n",
       " '.']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m,token in enumerate(token_list):\n",
    "    temp_token.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the token to fit the length requirement\n",
    "if len(temp_token) > max_len-1:\n",
    "    temp_token= temp_token[:max_len-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add [SEP] at the end\n",
    "temp_token.append('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'A',\n",
       " '##b',\n",
       " '##hi',\n",
       " '##jee',\n",
       " '##t',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'from',\n",
       " 'India',\n",
       " 'but',\n",
       " 'currently',\n",
       " 'live',\n",
       " 'in',\n",
       " 'USA',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'my',\n",
       " 'Office',\n",
       " 'laptop',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts.append(temp_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make id embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  1422  1271  1110   138  1830  3031 24260  1204   117   146  1821\n",
      "  1121  1726  1133  1971  1686  1107  3066   117  1142  1110  1139  3060\n",
      " 12574   119   102     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Make text token into id\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make mask embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine tune of predict, with token mask is 1,pad token is 0\n",
    "attention_masks = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "attention_masks[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make segmention type embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids = [[0] * len(input_id) for input_id in input_ids]\n",
    "segment_ids[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make embeddings into torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "segment_ids = torch.tensor(segment_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with model\n",
    "We only send input_ids embedding to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set save model to Evalue loop\n",
    "save_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predict result\n",
    "\n",
    "with torch.no_grad():\n",
    "        outputs = save_model(input_ids, token_type_ids=None,\n",
    "        attention_mask=None,)\n",
    "           \n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make logits into numpy type predict result\n",
    "# The predict result contain each token's all tags predict result\n",
    "predict_results = logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 45, 20)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make each token predict result into softmax mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_arrays_soft = softmax(predict_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.7480103e-08, 1.5235217e-07, 7.3533293e-08, 5.5031261e-08,\n",
       "       9.8893445e-08, 3.0766154e-07, 1.1635217e-07, 1.6125286e-07,\n",
       "       1.8131547e-07, 5.3372929e-08, 7.1269824e-08, 2.1476455e-07,\n",
       "       1.1722996e-07, 1.5373669e-07, 4.5835358e-07, 9.5748305e-08,\n",
       "       2.4343160e-07, 3.4811589e-07, 1.8879630e-01, 2.7725463e-07],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_arrays_soft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = result_arrays_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 20)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_array),len(result_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parser result\n",
    "Make the predict array into readable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get each token final predict tag index result\n",
    "result_list = np.argmax(result_array,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  9,  9,  9, 10, 17, 17, 17, 17,  9,  9,  9,  9,  0,  9,  9,  9,\n",
       "        9,  0,  9,  9,  9,  9, 10,  9,  9, 19,  9,  9,  9,  9,  9,  9,  9,\n",
       "        9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9], dtype=int64)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get token predict tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:[CLS]\n",
      "Predict_Tag:[CLS]\n",
      "\n",
      "Token:My\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:name\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:is\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:A\n",
      "Predict_Tag:B-org\n",
      "\n",
      "Token:##b\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##hi\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##jee\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:##t\n",
      "Predict_Tag:X\n",
      "\n",
      "Token:,\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:I\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:am\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:from\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:India\n",
      "Predict_Tag:B-geo\n",
      "\n",
      "Token:but\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:currently\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:live\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:in\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:USA\n",
      "Predict_Tag:B-geo\n",
      "\n",
      "Token:,\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:this\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:is\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:my\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:Office\n",
      "Predict_Tag:B-org\n",
      "\n",
      "Token:laptop\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:.\n",
      "Predict_Tag:O\n",
      "\n",
      "Token:[SEP]\n",
      "Predict_Tag:[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, mark in enumerate(attention_masks[0]):\n",
    "    if mark>0:\n",
    "        print(\"Token:%s\"%(temp_token[i]))\n",
    "#         print(\"Tag:%s\"%(result_list[i]))\n",
    "        print(\"Predict_Tag:%s\"%(tag2name[result_list[i]]))\n",
    "        #print(\"Posibility:%f\"%(result_array[i][result_list[i]]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
